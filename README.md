# ğŸš— Vehicle Data End-to-End MLOps Project

> **A complete production-grade Machine Learning system â€” from raw data ingestion to cloud deployment with CI/CD.**

This project demonstrates how to build a **scalable, modular, and industry-ready MLOps pipeline** using **Python, MongoDB, AWS, Docker, and GitHub Actions**.
It closely mirrors **real-world ML engineering workflows** followed in production environments.

---

## ğŸš€ What This Project Demonstrates

* âœ… End-to-end **ML pipeline architecture**
* âœ… Clean & modular **project template**
* âœ… **MongoDB Atlas** for real-world data ingestion
* âœ… **Schema-based data validation**
* âœ… Feature engineering & transformation pipelines
* âœ… **Model training, evaluation & versioning**
* âœ… **AWS S3-based model registry**
* âœ… **Dockerized ML application**
* âœ… **CI/CD pipeline using GitHub Actions**
* âœ… **Self-hosted GitHub Runner on EC2**
* âœ… **Production inference API with Flask**

---

## ğŸ§± Project Architecture

```
Raw Data (MongoDB)
        â†“
Data Ingestion
        â†“
Data Validation
        â†“
Data Transformation
        â†“
Model Training
        â†“
Model Evaluation
        â†“
Model Pusher (AWS S3)
        â†“
Prediction Pipeline (Flask API)
        â†“
Docker + CI/CD + EC2 Deployment
```

---

## ğŸ“ Project Structure

```bash
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/            # ML pipeline components
â”‚   â”œâ”€â”€ configuration/         # MongoDB & AWS configurations
â”‚   â”œâ”€â”€ constants/             # Global constants & configs
â”‚   â”œâ”€â”€ data_access/           # MongoDB data access logic
â”‚   â”œâ”€â”€ entity/                # Config & artifact entities
â”‚   â”œâ”€â”€ logger/                # Centralized logging
â”‚   â”œâ”€â”€ exception/             # Custom exception handling
â”‚   â”œâ”€â”€ aws_storage/           # S3 model push/pull logic
â”‚   â””â”€â”€ utils/                 # Utility functions
â”œâ”€â”€ notebook/                  # EDA & MongoDB demo notebooks
â”œâ”€â”€ templates/                 # HTML templates
â”œâ”€â”€ static/                    # CSS / static files
â”œâ”€â”€ app.py                     # Prediction API
â”œâ”€â”€ demo.py                    # Pipeline testing script
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .dockerignore
â””â”€â”€ .github/workflows/aws.yaml
```

---

## âš™ï¸ Local Project Setup

### 1ï¸âƒ£ Create Project Template

```bash
python template.py
```

---

### 2ï¸âƒ£ Configure Local Packages

* Implement local package imports using:

  * `setup.py`
  * `pyproject.toml`
* ğŸ“˜ Reference file: `crashcourse.txt`

---

### 3ï¸âƒ£ Create Virtual Environment

```bash
conda create -n vehicle python=3.10 -y
conda activate vehicle
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Verify Installation

```bash
pip list
```

âœ” Confirms local project packages are installed correctly.

---

## ğŸƒ MongoDB Atlas Setup

1. Create an account on **MongoDB Atlas**
2. Create a new project â†’ Create Cluster (M0 â€“ Free Tier)
3. Create a **Database User**
4. Add Network Access:

   ```
   0.0.0.0/0
   ```
5. Get Python connection string
6. Replace password and store securely

---

### MongoDB Data Flow

* Dataset placed inside `notebook/`
* Data pushed using `mongoDB_demo.ipynb`
* Verified via:

  ```
  MongoDB Atlas â†’ Database â†’ Browse Collections
  ```

---

## ğŸ§  Logging, Exception Handling & EDA

* Centralized logging module
* Custom exception handling
* Tested via `demo.py`
* EDA & Feature Engineering notebooks included

---

## ğŸ”„ Data Ingestion Pipeline

* MongoDB â†’ Python â†’ Pandas DataFrame
* Config-driven ingestion
* Artifact tracking enabled

### Environment Variable Setup

**Bash**

```bash
export MONGODB_URL="mongodb+srv://<username>:<password>@..."
```

**PowerShell**

```powershell
$env:MONGODB_URL="mongodb+srv://<username>:<password>@..."
```

ğŸ“Œ `artifact/` directory is ignored via `.gitignore`

---

## âœ… Data Validation & Transformation

* Schema validation using `schema.yaml`
* Data drift & missing value checks
* Feature engineering pipeline
* Reusable transformers

---

## ğŸ¤– Model Training

* Custom estimator classes
* Config-driven training
* Model artifacts stored
* Threshold-based model comparison

---

## ğŸ“Š Model Evaluation & Registry (AWS S3)

* AWS IAM user setup
* S3 bucket for model registry
* Automatic comparison with previous models
* Only better models get pushed

### Required Constants

```python
MODEL_EVALUATION_CHANGED_THRESHOLD_SCORE = 0.02
MODEL_BUCKET_NAME = "my-model-mlopsproj"
MODEL_PUSHER_S3_KEY = "model-registry"
```

---

## ğŸŒ Prediction Pipeline

* Flask-based inference API
* `/predict` â†’ Real-time predictions
* `/training` â†’ Trigger model training

---

## ğŸ³ Docker & CI/CD Pipeline

* Dockerized application
* GitHub Actions workflow
* Self-hosted GitHub Runner on EC2
* AWS ECR for Docker image storage

---

## â˜ï¸ Deployment on AWS EC2

* Ubuntu 24.04 EC2 instance
* Docker installed on server
* Port **5000** enabled

### Access the Application

```
http://<EC2_PUBLIC_IP>:5000
```

---

## ğŸ” GitHub Secrets Used

* `AWS_ACCESS_KEY_ID`
* `AWS_SECRET_ACCESS_KEY`
* `AWS_DEFAULT_REGION`
* `ECR_REPO`

---

## ğŸ Final Outcome

âœ” Fully automated ML lifecycle
âœ” Production-ready deployment
âœ” Real-world MLOps practices
âœ” Scalable & maintainable codebase

---

## ğŸ™Œ Author

**Mohd Faizan**
Data Scientist | Machine Learning Engineer
Focused on **Deep Learning, MLOps & Production ML Systems**

---

â­ **If you like this project, donâ€™t forget to star the repo!**
